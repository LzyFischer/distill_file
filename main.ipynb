{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bea07901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'N/A'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_alphabet_choice(text):\n",
    "  if not text:\n",
    "    return \"N/A\"\n",
    "  # pdb.set_trace()\n",
    "  match = re.findall(r\"answer is \\((A|B|C|D|E|F)\\)\", text)\n",
    "  if match:\n",
    "    return match[-1]\n",
    "  else:\n",
    "    match = re.findall(r\"\\((A|B|C|D|E|F)\\)\", text)\n",
    "    return match[-1] if match else \"N/A\"\n",
    "\n",
    "text = 'Read the problem carefully. Break it into small subâ€‘problems. Solve each subâ€‘problem step by step. Doubleâ€‘check the answers.\\n\\nGiven the following question and 4 candidate answers (A, B, C, D), choose the best answer.\\nQuestion: An astronomer observes that a planet rotates faster after a meteorite impact. Which is the most likely effect of this increase in rotation?\\nA. Planetary density will decrease.\\nB. Planetary years will become longer.\\nC. Planetary days will become shorter.\\nD. Planetary gravity will become stronger.\\nPlease conclude with your choice. Your response should end with \"The best answer is [LETTER]\" where LETTER is one of A, B, C, D.\\n\\nThe best answer is C. Planetary days will become shorter.\\n\\nExplanation:\\n\\n1. The question asks about the effect of an increase in rotation of a planet.\\n2. Rotation refers to the spinning motion of the planet on its axis.\\n3. A day on a planet is defined as the time it takes for the planet to complete one full rotation on its axis.\\n4. Therefore, an increase in rotation would result in a decrease in the time it takes for the planet to complete one full rotation, which means the planet\\'s days will become shorter.\\n5. The other options do not directly relate to the increase in rotation:\\n   a. Planetary density is not directly related to rotation.\\n   b. Planetary years are not directly related to rotation.\\n   c. Planetary gravity is not directly related to rotation, but it could be affected by changes in the planet\\'s mass or shape, which are not mentioned in the question. However, an increase in rotation would not necessarily cause an increase in gravity.\\n\\nTherefore, the best answer is C. Planetary days will become shorter.'\n",
    "get_alphabet_choice(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6311f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0956aafb",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      3\u001b[0m latex_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mboxed\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mleft(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfrac\u001b[39m\u001b[38;5;132;01m{60}\u001b[39;00m\u001b[38;5;132;01m{65}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mfrac\u001b[39m\u001b[38;5;132;01m{20}\u001b[39;00m\u001b[38;5;132;01m{13}\u001b[39;00m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mright)}\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m match \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mboxed\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43m(.*?)\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m}\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfindall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatex_string\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# match = re.search(pattern, latex_string)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m match:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "latex_string = r\"\\boxed{\\left(\\frac{60}{65}, \\frac{20}{13}\\right)}\"\n",
    "match = re.compile(\"\\\\boxed\\{(.*?)\\}\").findall(latex_string)[-1]\n",
    "# match = re.search(pattern, latex_string)\n",
    "\n",
    "if match:\n",
    "    extracted_string = match.group(1)\n",
    "    print(extracted_string)\n",
    "else:\n",
    "    print(\"No match found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/uvadm/zhenyu/miniconda3/envs/distill/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/project/uvadm/zhenyu/miniconda3/envs/distill/lib/python3.10/site-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "/project/uvadm/zhenyu/miniconda3/envs/distill/lib/python3.10/site-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] CE_A=6.520e+00  CE_B=6.527e+00 |âˆ‡|_A=0.000e+00  |âˆ‡|_B=4.512e-04\n",
      "[epoch 2] CE_A=6.528e+00  CE_B=6.530e+00 |âˆ‡|_A=3.428e-06  |âˆ‡|_B=4.342e-04\n",
      "[epoch 3] CE_A=6.528e+00  CE_B=6.528e+00 |âˆ‡|_A=7.188e-06  |âˆ‡|_B=5.402e-04\n",
      "[epoch 4] CE_A=6.535e+00  CE_B=6.535e+00 |âˆ‡|_A=1.286e-05  |âˆ‡|_B=6.354e-04\n",
      "[epoch 5] CE_A=6.531e+00  CE_B=6.530e+00 |âˆ‡|_A=1.190e-05  |âˆ‡|_B=4.699e-04\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# test_two_lora_cuda_v2.py  â€“ multi-epoch, harder prompts\n",
    "# --------------------------------------------------------\n",
    "import torch, random, math\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig\n",
    "\n",
    "BASE_MODEL   = \"hf-internal-testing/tiny-random-GPT2LMHeadModel\"\n",
    "DEVICE       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TEMPERATURE  = 1.0\n",
    "LAMBDA_KL    = 0.5\n",
    "EPOCHS       = 5\n",
    "BATCH_SIZE   = 4               # keep small â€“ tiny model has 50257-token vocab\n",
    "\n",
    "# -- build model + two LoRA adapters ------------------------\n",
    "cfg = dict(\n",
    "    r=8, lora_alpha=32, lora_dropout=0.05, target_modules=[\"c_attn\"]\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(BASE_MODEL).to(DEVICE)\n",
    "model.add_adapter(LoraConfig(**cfg), adapter_name=\"A\")\n",
    "model.add_adapter(LoraConfig(**cfg), adapter_name=\"B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # needed for padding\n",
    "\n",
    "# -- pool of â€œdifficultâ€ prompts ----------------------------\n",
    "PROMPT_POOL = [\n",
    "    # odd capitalization, rare words, and LaTeX\n",
    "    \"Incongruous rhythms of Î³-oscillation juxtapose with d_{i,j}=âˆ‘_k Î±_k\",\n",
    "    # mixed languages & emoji\n",
    "    \"Quand je dis Â« superposition Â», æˆ‘å¹¶ä¸æ˜¯ ðŸ¤” talking about furniture.\",\n",
    "    # long numeric / scientific string\n",
    "    \"The half-life of Â¹Â³â·Cs is 30.17 years; estimate its activity after 9.2Ã—10â¸ seconds.\",\n",
    "    # code-like fragment\n",
    "    \"def paradox(x): return (lambda y: y(y))(lambda y: y(y))(x)\",\n",
    "    # contradiction\n",
    "    \"The smallest prime number larger than 100 is obviously 99.\",\n",
    "    # heavy punctuation\n",
    "    \"Waitâ€”did he just say, â€œalgorithmic *entropy* equals âˆ‘-p logâ‚‚ pâ€?!\",\n",
    "    # archaic phrasing\n",
    "    \"Hark! what resplendent algorithm bewitches yon silicon steed?\",\n",
    "    # deliberately misspelled tokens\n",
    "    \"Qantum mechanicz deskribe probabilistic superpostions.\",\n",
    "]\n",
    "\n",
    "def random_batch(batch_size):\n",
    "    prompts = random.sample(PROMPT_POOL, batch_size)\n",
    "    data = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    data[\"labels\"] = data[\"input_ids\"].clone()\n",
    "    return data\n",
    "\n",
    "def forward_two_adapters(m, **kw):\n",
    "    m.set_adapter(\"A\"); out_a = m(**kw)\n",
    "    m.set_adapter(\"B\"); out_b = m(**kw)\n",
    "    return out_a.logits, out_b.logits\n",
    "\n",
    "def grad_mag(tag):\n",
    "    g = [p.grad.abs().mean() for n,p in model.named_parameters()\n",
    "         if tag in n and p.grad is not None]\n",
    "    return float(torch.stack(g).mean()) if g else 0.0\n",
    "\n",
    "# --------------- training loop -----------------------------\n",
    "model.train()\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    batch = random_batch(BATCH_SIZE)\n",
    "    logits_a, logits_b = forward_two_adapters(model, **batch)\n",
    "\n",
    "    loss_ce_a = F.cross_entropy(\n",
    "        logits_a.view(-1, logits_a.size(-1)),\n",
    "        batch[\"labels\"].view(-1)\n",
    "    )\n",
    "    loss_ce_b = F.cross_entropy(\n",
    "        logits_b.view(-1, logits_b.size(-1)),\n",
    "        batch[\"labels\"].view(-1)\n",
    "    )\n",
    "\n",
    "    loss_kl = F.kl_div(\n",
    "        F.log_softmax(logits_a / TEMPERATURE, dim=-1),\n",
    "        F.softmax(logits_b.detach() / TEMPERATURE, dim=-1),\n",
    "        reduction=\"batchmean\"\n",
    "    )\n",
    "    loss = loss_ce_a + loss_ce_b + LAMBDA_KL * loss_kl \n",
    "\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    print(f\"[epoch {epoch}] \"\n",
    "          f\"CE_A={loss_ce_a.item():.3e}  CE_B={loss_ce_b.item():.3e} \"\n",
    "          f\"|âˆ‡|_A={grad_mag('lora_A'):.3e}  |âˆ‡|_B={grad_mag('lora_B'):.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f9afd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping invalid JSON line: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mReasoningDataset\u001b[39;00m(\u001b[43mDataset\u001b[49m):\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    Each Item is a dict with keys\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, tokenizer):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb81d68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer and base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:28<00:00, 14.09s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####### Tokenizer & Base Model #######\n",
    "print(\"Loading tokenizer and base model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "for p in base_model.parameters():\n",
    "    p.requires_grad = False\n",
    "base_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91e03d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Collate Function ########\n",
    "def collate_fn(batch):\n",
    "    ins, cots = zip(*batch)\n",
    "    out = {k: [] for k in [\n",
    "        'enc_ids', 'enc_mask', 'gen_ids', 'gen_mask', 'labels'\n",
    "    ]}\n",
    "\n",
    "    for inp, cot in zip(ins, cots):\n",
    "        e = [tokenizer(inp + c, return_tensors='pt',\n",
    "                        truncation=True, max_length=1024) for c in cot]\n",
    "        g = tokenizer(inp, return_tensors='pt',\n",
    "                        padding='max_length',\n",
    "                        truncation=True, max_length=1024)\n",
    "        L = len(g['input_ids'][0])\n",
    "\n",
    "        lab = [tokenizer(inp + c, return_tensors='pt',\n",
    "                        truncation=True, max_length=1024)['input_ids'] for c in cot]\n",
    "                        \n",
    "        for i in range(len(lab)):\n",
    "            lab[i][:, L:] = -100\n",
    "\n",
    "        for k, v in zip(\n",
    "            ['enc_ids1', 'enc_mask1', 'enc_ids2', 'enc_mask2', 'gen_ids', 'gen_mask', 'labels1', 'labels2'],\n",
    "            [e1['input_ids'], e1['attention_mask'], e2['input_ids'], e2['attention_mask'], g['input_ids'], g['attention_mask'], lab1, lab2]\n",
    "        ):\n",
    "            out[k].append(v)\n",
    "\n",
    "    for k in out: out[k] = torch.cat(out[k], dim=0)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa5c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####### add datasets ######\n",
    "train_set = ReasoningDataset(...)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=4)\n",
    "test_set = EvalDataset(load_jsonl(TEST_PATH))\n",
    "test_loader = DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=4)\n",
    "\n",
    "######### Model #########\n",
    "student_1 = get_peft_model(\n",
    "    copy.deepcopy(base_model),\n",
    "    LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=['q_proj', 'v_proj']\n",
    "    )\n",
    ")\n",
    "student_2 = get_peft_model(\n",
    "    copy.deepcopy(base_model),\n",
    "    LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=['q_proj', 'v_proj']\n",
    "    )\n",
    ")\n",
    "\n",
    "# router\n",
    "class Router():\n",
    "    \"\"\"\n",
    "    input: hiddens n * d\n",
    "    output: logits num_stu * 1\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def forward(self, hiddens):\n",
    "        return logits\n",
    "\n",
    "# encoder\n",
    "class CoTEncoder():\n",
    "    \"\"\"\n",
    "    input: prompt + cot\n",
    "    output: hiddens n * d\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return hiddens\n",
    "\n",
    "class WeightLearner():\n",
    "    \"\"\"\n",
    "    input: hiddens n * d\n",
    "    output: logits 1\n",
    "    \"\"\"\n",
    "    def __init__(self, hiddens):\n",
    "        self.hiddens = hiddens\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        return logits\n",
    "\n",
    "class DistillModel():\n",
    "    \"\"\"input: prompt + cot\n",
    "    output: loss\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.cot_encoder = CoTEncoder(model)\n",
    "        self.router = Router(model)\n",
    "        self.weight_learner = WeightLearner(model)\n",
    "        self.temperature = TEMPERATURE\n",
    "        self.kl_weight = KL_WEIGHT\n",
    "        self.num_stu = 2\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: dict with keys\n",
    "            enc_ids1, enc_mask1, enc_ids2, enc_mask2, gen_ids, gen_mask, labels1, labels2\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        hiddens_1 = self.cot_encoder.forward(inputs['enc_ids1'], inputs['enc_mask1'])\n",
    "        hiddens_2 = self.cot_encoder.forward(inputs['enc_ids2'], inputs['enc_mask2'])\n",
    "        router_logits_1 = self.router.forward(hiddens_1)\n",
    "        router_logits_2 = self.router.forward(hiddens_2)\n",
    "\n",
    "        # choose which student to use given router_logits\n",
    "        logits_1 = \n",
    "\n",
    "        logits = self.router.forward(inputs['enc_ids1'], inputs['enc_mask1'])\n",
    "        loss = F.cross_entropy(logits, inputs['labels1'])\n",
    "        # KL loss\n",
    "        hiddens = self.cot_encoder.forward(inputs['enc_ids2'], inputs['enc_mask2'])\n",
    "        logits = self.weight_learner.forward(hiddens)\n",
    "        loss += self.kl_weight * F.kl_div(\n",
    "            F.log_softmax(logits / self.temperature, dim=-1),\n",
    "            F.softmax(logits / self.temperature, dim=-1),\n",
    "            reduction='batchmean'\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Trainer():\n",
    "    def __init__(self, model, train_loader, test_loader):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "    \n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            for batch in tqdm(self.train_loader):\n",
    "                # forward pass\n",
    "                logits = self.model.forward(batch['enc_ids1'], batch['enc_mask1'])\n",
    "                loss = F.cross_entropy(logits, batch['labels1'])\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.test_loader):\n",
    "                # forward pass\n",
    "                logits = self.model.forward(batch['enc_ids1'], batch['enc_mask1'])\n",
    "                loss = F.cross_entropy(logits, batch['labels1'])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3627c4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project/uvadm/zhenyu/miniconda3/envs/distill/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer & frozen backbone â€¦\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:18<00:00,  9.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building two LoRA students â€¦\n",
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.0470\n",
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.0470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/1772 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 159 but got size 287 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 195\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, NUM_EPOCHS\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    194\u001b[0m     prog \u001b[38;5;241m=\u001b[39m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, dynamic_ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 195\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m prog:\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;66;03m# â”€ move to device â”€\u001b[39;00m\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m batch: batch[k] \u001b[38;5;241m=\u001b[39m batch[k]\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    198\u001b[0m         gen_ids, labels1, labels2 \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels1\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/project/uvadm/zhenyu/miniconda3/envs/distill/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/project/uvadm/zhenyu/miniconda3/envs/distill/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/project/uvadm/zhenyu/miniconda3/envs/distill/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/project/uvadm/zhenyu/miniconda3/envs/distill/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 116\u001b[0m, in \u001b[0;36mcollate_fn\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    108\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menc_ids1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menc_mask1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menc_ids2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menc_mask2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    109\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels2\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m          g[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],  g[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    113\u001b[0m          lab1, lab2]):\n\u001b[1;32m    114\u001b[0m         out[k]\u001b[38;5;241m.\u001b[39mappend(v)\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m out: out[k] \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 159 but got size 287 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "# main.py  â”€â”€ two-student MoE with CoT routing + weighted ensemble KL\n",
    "import os, json, random, re, copy, torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DATA_PATH       = \"data/gsm8k/train/train_cot_distill.jsonl\"\n",
    "DEV_PATH        = \"data/gsm8k/train/test_cot_distill.jsonl\"\n",
    "TEST_PATH       = \"data/cot_response.jsonl\"\n",
    "BASE_MODEL      = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "BATCH_SIZE      = 4\n",
    "NUM_EPOCHS      = 2\n",
    "LR              = 1e-5\n",
    "\n",
    "alpha           = 0.5     # unused (was ensemble weight in baseline)\n",
    "temperature     = 1.0\n",
    "kl_weight       = 0.5\n",
    "USE_KL_DISTILLATION = True\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cuda.matmul.allow_tf32 = True      # (optional) speed\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HELPERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                json_object = json.loads(line)\n",
    "                data.append(json_object)\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"Skipping invalid JSON line: {line.strip()}\")\n",
    "    return data\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DATASETS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class ReasoningDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Returns: instruction, two sampled CoTs, label\n",
    "    The two CoTs are later routed to two students.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        instruction = item['prompt']\n",
    "        cot = item['messages'][1]['content']\n",
    "        cot1, cot2 = [cot, cot]\n",
    "        return instruction, cot1, cot2, None\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, data): self.data = data\n",
    "    def __len__(self): return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        d = self.data[idx]\n",
    "        return d[\"question\"], d[\"answer\"]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TOKENIZER & BASE MODEL â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"Loading tokenizer & frozen backbone â€¦\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "for p in base_model.parameters(): p.requires_grad = False\n",
    "base_model.eval()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ COLLATE FN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def collate_fn(batch):\n",
    "    ins, cot1, cot2, _ = zip(*batch)                 # we don't need labels here\n",
    "    out = {k: [] for k in [\n",
    "        \"enc_ids1\",\"enc_mask1\",\"enc_ids2\",\"enc_mask2\",\n",
    "        \"gen_ids\",\"gen_mask\",\"labels1\",\"labels2\"\n",
    "    ]}\n",
    "\n",
    "    for inp, c1, c2 in zip(ins, cot1, cot2):\n",
    "        # â€” tokens for the encoder (instruction + CoT) â€”\n",
    "        e1 = tokenizer(inp + c1, return_tensors=\"pt\",\n",
    "                       truncation=True, max_length=1024)\n",
    "        e2 = tokenizer(inp + c2, return_tensors=\"pt\",\n",
    "                       truncation=True, max_length=1024)\n",
    "\n",
    "        # â€” instruction-only prompt for generation â€”\n",
    "        g  = tokenizer(inp, return_tensors=\"pt\",\n",
    "                       padding=\"max_length\",\n",
    "                       truncation=True, max_length=1024)\n",
    "        L  = len(g[\"input_ids\"][0])        # prompt length\n",
    "\n",
    "        # â€” labels (mask out prompt tokens) â€”\n",
    "        lab1 = tokenizer(inp + c1, return_tensors=\"pt\",\n",
    "                         padding=\"max_length\",\n",
    "                         truncation=True, max_length=1024)[\"input_ids\"]\n",
    "        lab2 = tokenizer(inp + c2, return_tensors=\"pt\",\n",
    "                         padding=\"max_length\",\n",
    "                         truncation=True, max_length=1024)[\"input_ids\"]\n",
    "        lab1[:, :L] = -100\n",
    "        lab2[:, :L] = -100\n",
    "\n",
    "        for k, v in zip(\n",
    "            [\"enc_ids1\",\"enc_mask1\",\"enc_ids2\",\"enc_mask2\",\n",
    "             \"gen_ids\",\"gen_mask\",\"labels1\",\"labels2\"],\n",
    "            [e1[\"input_ids\"], e1[\"attention_mask\"],\n",
    "             e2[\"input_ids\"], e2[\"attention_mask\"],\n",
    "             g[\"input_ids\"],  g[\"attention_mask\"],\n",
    "             lab1, lab2]):\n",
    "            out[k].append(v)\n",
    "\n",
    "    for k in out: out[k] = torch.cat(out[k], dim=0)\n",
    "    return out\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ NEW MODULES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class CotEncoder(nn.Module):\n",
    "    \"\"\"Frozen encoder that pools last hidden state.\"\"\"\n",
    "    def __init__(self, backbone, pool=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.backbone, self.pool = backbone, pool\n",
    "    @torch.no_grad()\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        h = self.backbone.model(\n",
    "            input_ids     = input_ids,\n",
    "            attention_mask= attention_mask,\n",
    "            output_hidden_states=True,\n",
    "            return_dict=True\n",
    "        ).hidden_states[-1]                     # [B,T,d]\n",
    "        if self.pool == \"mean\":\n",
    "            num = (h * attention_mask.unsqueeze(-1)).sum(1)\n",
    "            den = attention_mask.sum(1, keepdim=True)\n",
    "            return (num / den).float()          # [B,d]\n",
    "        return h[:,0].float()\n",
    "\n",
    "class Router(nn.Module):\n",
    "    \"\"\"Two-way hard router with straight-through Gumbel-Softmax.\"\"\"\n",
    "    def __init__(self, d_model, tau=1.0):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model//2),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model//2, 2)\n",
    "        )\n",
    "        self.tau = tau\n",
    "    def forward(self, h):       # h:[B,d]\n",
    "        return F.gumbel_softmax(self.mlp(h), tau=self.tau, hard=True)\n",
    "\n",
    "class WeightLearner(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "    def forward(self, h):       # h:[B,d]\n",
    "        return torch.sigmoid(self.fc(h)).squeeze(-1)   # [B]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LoRA STUDENTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def make_student(backbone):\n",
    "    model = copy.deepcopy(backbone)\n",
    "    cfg   = LoraConfig(\n",
    "        r=8, lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=0.05,\n",
    "        bias=\"none\", task_type=TaskType.CAUSAL_LM\n",
    "    )\n",
    "    model = get_peft_model(model, cfg)\n",
    "    model.print_trainable_parameters()\n",
    "    return model\n",
    "\n",
    "print(\"Building two LoRA students â€¦\")\n",
    "student1, student2 = make_student(base_model), make_student(base_model)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ INSTANTIATE NEW UTILS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "encoder     = CotEncoder(base_model).to(DEVICE)\n",
    "router      = Router(base_model.config.hidden_size).to(DEVICE)\n",
    "weight_net  = WeightLearner(base_model.config.hidden_size).to(DEVICE)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ DATA LOADERS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "train_set = ReasoningDataset(load_data(DATA_PATH), tokenizer)\n",
    "train_loader = DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OPTIMIZER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "trainables = (list(student1.parameters()) +\n",
    "              list(student2.parameters()) +\n",
    "              list(router.parameters())   +\n",
    "              list(weight_net.parameters()))\n",
    "optimizer = torch.optim.AdamW(trainables, lr=LR)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ TRAIN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for epoch in range(1, NUM_EPOCHS+1):\n",
    "    prog = tqdm(train_loader, desc=f\"Epoch {epoch}\", dynamic_ncols=True)\n",
    "    for batch in prog:\n",
    "        # â”€ move to device â”€\n",
    "        for k in batch: batch[k] = batch[k].to(DEVICE)\n",
    "        gen_ids, labels1, labels2 = batch[\"gen_ids\"], batch[\"labels1\"], batch[\"labels2\"]\n",
    "\n",
    "        # â”€ encode both CoTs â”€\n",
    "        h1 = encoder(batch[\"enc_ids1\"], batch[\"enc_mask1\"])\n",
    "        h2 = encoder(batch[\"enc_ids2\"], batch[\"enc_mask2\"])\n",
    "\n",
    "        # â”€ routing decisions â”€\n",
    "        gate1 = router(h1)          # [B,2] one-hot\n",
    "        gate2 = router(h2)\n",
    "        m1 = (gate1[:,0] | gate2[:,0]).bool()   # mask for student-1\n",
    "        m2 = (gate1[:,1] | gate2[:,1]).bool()   # mask for student-2\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # â”€ student-1 forward â”€\n",
    "        if m1.any():\n",
    "            out1 = student1(input_ids=gen_ids[m1], labels=labels1[m1])\n",
    "            logits1 = torch.zeros((*labels1.shape, student1.config.vocab_size),\n",
    "                                  dtype=out1.logits.dtype, device=DEVICE)\n",
    "            logits1[m1] = out1.logits\n",
    "            ce1 = out1.loss\n",
    "        else:\n",
    "            logits1 = torch.zeros((*labels1.shape, base_model.config.vocab_size),\n",
    "                                  dtype=torch.float16, device=DEVICE)\n",
    "            ce1 = torch.tensor(0., device=DEVICE)\n",
    "\n",
    "        # â”€ student-2 forward â”€\n",
    "        if m2.any():\n",
    "            out2 = student2(input_ids=gen_ids[m2], labels=labels2[m2])\n",
    "            logits2 = torch.zeros_like(logits1)\n",
    "            logits2[m2] = out2.logits\n",
    "            ce2 = out2.loss\n",
    "        else:\n",
    "            logits2 = torch.zeros_like(logits1)\n",
    "            ce2 = torch.tensor(0., device=DEVICE)\n",
    "\n",
    "        # â”€ learn sample weight w âˆˆ (0,1) â”€\n",
    "        w = weight_net((h1 + h2) / 2).view(-1,1,1)     # [B,1,1]\n",
    "        logits_ens = w * logits1 + (1 - w) * logits2\n",
    "\n",
    "        # â”€ KL divergence against ensemble â”€\n",
    "        if USE_KL_DISTILLATION:\n",
    "            logp1 = F.log_softmax(logits1 / temperature, dim=-1)\n",
    "            logp2 = F.log_softmax(logits2 / temperature, dim=-1)\n",
    "            logpE = F.log_softmax(logits_ens.detach() / temperature, dim=-1)\n",
    "\n",
    "            tok_mask = (labels1 != -100)\n",
    "            kl1 = F.kl_div(logp1, logpE, reduction=\"none\",\n",
    "                           log_target=True).sum(-1)[tok_mask].mean()\n",
    "            kl2 = F.kl_div(logp2, logpE, reduction=\"none\",\n",
    "                           log_target=True).sum(-1)[tok_mask].mean()\n",
    "            loss = ce1 + ce2 + kl_weight * (kl1 + kl2)\n",
    "        else:\n",
    "            kl1 = kl2 = torch.tensor(0., device=DEVICE)\n",
    "            loss = ce1 + ce2\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        prog.set_postfix({\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"CE1\":  f\"{ce1.item():.3f}\",\n",
    "            \"CE2\":  f\"{ce2.item():.3f}\",\n",
    "            \"KL\":   f\"{(kl1+kl2).item():.3f}\"\n",
    "        })\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SIMPLE EVAL (router â†’ one student) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def extract_number(text):\n",
    "    nums = re.findall(r\"[-+]?\\d*\\.\\d+|\\d+\", text)\n",
    "    return nums[-1] if nums else None\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(dataset_path):\n",
    "    data = load_data(dataset_path)\n",
    "    loader = DataLoader(EvalDataset(data), batch_size=1)\n",
    "    correct = total = 0\n",
    "    for ins, label in tqdm(loader, desc=f\"Eval {os.path.basename(dataset_path)}\"):\n",
    "        prompt = ins[0]\n",
    "        e = tokenizer(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        h = encoder(e[\"input_ids\"], e[\"attention_mask\"])\n",
    "        gate = router(h)[0]              # [2]\n",
    "        if gate[0] == 1:\n",
    "            outs = student1.generate(**e, max_length=256)\n",
    "        else:\n",
    "            outs = student2.generate(**e, max_length=256)\n",
    "        pred_ans = extract_number(tokenizer.decode(outs[0], skip_special_tokens=True))\n",
    "        gold_ans = extract_number(label[0])\n",
    "        correct += (pred_ans == gold_ans)\n",
    "        total   += 1\n",
    "    acc = correct / total\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "\n",
    "evaluate(TEST_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "distill",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
